{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Load lifesciences domain content into Qdrant\n",
    "\n",
    "- an overly simple starting point\n",
    "- a public bucket with read-only access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-14 09:04:45    1567312 2311.08526v1.pdf\n",
      "2025-01-14 09:04:47     329278 2406.12925v2.pdf\n",
      "2025-01-13 13:53:19    3165662 2406.13106v3.pdf\n",
      "2025-01-14 09:04:43     505260 2409.12656v1.pdf\n",
      "2025-01-14 09:04:44     553008 2410.05046v1.pdf\n",
      "2025-01-14 09:04:47    3813091 2501.03172v1.pdf\n",
      "2025-01-13 13:53:20     708813 pmid24378760.pdf\n",
      "2025-01-13 13:53:21    3138069 pmid27438146.pdf\n",
      "2025-01-13 13:53:23    1771239 pmid27453043.pdf\n",
      "2025-01-13 13:53:12     845943 pmid30762338_si.pdf\n",
      "2025-01-13 13:53:13    2357171 pmid30862715.pdf\n",
      "2025-01-13 13:53:15    2663787 pmid33077733.pdf\n",
      "2025-01-13 13:53:16     891075 pmid35559673.pdf\n",
      "2025-01-13 16:31:13    4102905 s41587-021-01145-6.pdf\n",
      "2025-01-13 16:31:11    4550676 s41597-023-01960-3.pdf\n",
      "2025-01-13 13:53:17    1514184 s41698-024-00583-0.pdf\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls s3://kg-rememberall/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain-text-splitters langchain-qdrant langchain-community langchain-openai qdrant-client PyMuPDF boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from qdrant_client import QdrantClient\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "\n",
    "# Environment variable setup\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "QDRANT_API_KEY = os.getenv(\"QDRANT_API_KEY\")\n",
    "QDRANT_API_URL = os.getenv(\"QDRANT_API_URL\")  # Ensure this is correct\n",
    "\n",
    "# Set AWS credentials\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "os.environ[\"AWS_DEFAULT_REGION\"] = 'us-west-2'\n",
    "\n",
    "# Parameters\n",
    "COLLECTION_NAME = \"life_sciences_pdfs\"\n",
    "CHUNK_SIZE = 1000\n",
    "CHUNK_OVERLAP = 200\n",
    "\n",
    "# Initialize S3 client\n",
    "s3_client = boto3.client('s3', config=boto3.session.Config(signature_version='s3v4'))\n",
    "bucket_name = 'kg-rememberall'\n",
    "\n",
    "# Initialize the embedding model\n",
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://dcc50e13-4537-4069-9b9f-26da8f65900c.us-east4-0.gcp.cloud.qdrant.io\n"
     ]
    }
   ],
   "source": [
    "print(QDRANT_API_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qdrant client initialized for querying.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Qdrant client\n",
    "client = QdrantClient(\n",
    "    url=QDRANT_API_URL,\n",
    "    api_key=QDRANT_API_KEY,\n",
    "    prefer_grpc=True,  # Use gRPC for faster communication\n",
    "    timeout=30  # Set a timeout to avoid hanging\n",
    ")\n",
    "\n",
    "qdrant = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    embedding=embedding_model,\n",
    ")\n",
    "\n",
    "print(\"Qdrant client initialized for querying.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qdrant client initialized for querying.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Qdrant client\n",
    "client = QdrantClient(\n",
    "    url=QDRANT_API_URL,\n",
    "    api_key=QDRANT_API_KEY,\n",
    "    prefer_grpc=True,  # Use gRPC for faster communication\n",
    "    timeout=30  # Set a timeout to avoid hanging\n",
    ")\n",
    "\n",
    "# Initialize the QdrantVectorStore for querying\n",
    "qdrant = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    embedding=embedding_model  # Correct parameter name\n",
    ")\n",
    "\n",
    "print(\"Qdrant client initialized for querying.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result 1:\n",
      "ﬁles across hundreds of cell lines4–7, machine learning (ML) models have\n",
      "emerged as a promising approach towards predicting drug response8–13. ML\n",
      "models for drug response prediction typically integrate omic data from\n",
      "cancercelllineswithdrugproﬁlestopredictdrugsensitivity,asmeasuredby\n",
      "IC50 or AUC13,14.\n",
      "Several studies have so far addressed open questions on how to train\n",
      "ML models for drug response prediction. Notably, Shariﬁ-Noghabi et al.14\n",
      "carried out a systematic study on the comparative performance of several\n",
      "ML models when trained and tested on the most popular cell line datasetsto\n",
      "predict different measures of drug response. In agreement with previously\n",
      "reported striking discordances between two large pharmacogenomic\n",
      "datasets15, namely CGP6 and CCLE5, cross-domain generalization issues\n",
      "that question the application of ML models in clinically relevant tasks have\n",
      "been reported14. The use of IC50 as a proxy of therapeutic efﬁcacy has also\n",
      "\n",
      "Result 2:\n",
      "was covered by only two drugs, indicating a significant gap for\n",
      "further research. Our network medicine framework, empowered\n",
      "by GenAI, shows promise in identifying drug combinations with\n",
      "a high degree of specificity, knowing the exact signaling pathways\n",
      "and proteins that serve as targets. It is noteworthy that ChatGPT\n",
      "successfully accelerated the process of identifying drug mentions\n",
      "in clinical trials, though further investigations are required to\n",
      "determine the relationships among the drug mentions.\n",
      "Index Terms—Network Medicine, Drug Repurposing, Gener-\n",
      "ative AI, LLMs, Multilayered Network, Signaling Pathways\n",
      "I. INTRODUCTION\n",
      "A. Highlights\n",
      "1.\n",
      "Presenting a multilayered network medicine approach\n",
      "for complex diseases, empowered by Generative AI,\n",
      "that produces drug repurposing combinations from real-\n",
      "world evidence.\n",
      "2.\n",
      "Introducing a highly configurable learning algorithm that\n",
      "harnesses ChatGPT prompt engineering, executed on the\n",
      "fly to analyze each data item (clinical trials) with the\n",
      "\n",
      "Result 3:\n",
      "14\n",
      "Scientific Data |           (2023) 10:67  | https://doi.org/10.1038/s41597-023-01960-3\n",
      "www.nature.com/scientificdata\n",
      "www.nature.com/scientificdata/\n",
      "\t 19.\t Wu, C., Gudivada, R. C., Aronow, B. J. & Jegga, A. G. Computational drug repositioning through heterogeneous network \n",
      "clustering. BMC Systems Biology 7, S6 (2013).\n",
      "\t 20.\t Dudley, J. T., Deshpande, T. & Butte, A. J. Exploiting drug-disease relationships for computational drug repositioning. Briefings in \n",
      "Bioinformatics 12, 303–311 (2011).\n",
      "\t 21.\t Xu, R. & Wang, Q. Large-scale extraction of accurate drug-disease treatment pairs from biomedical literature for drug repurposing. \n",
      "BMC Bioinformatics 14, 181 (2013).\n",
      "\t 22.\t Lin, X., Li, X. & Lin, X. A review on applications of computational methods in drug screening and design. Molecules 25, 1375 \n",
      "(2020).\n",
      "\t 23.\t Dai, Y.-F. & Zhao, X.-M. A survey on the computational approaches to identify drug targets in the postgenomic era. BioMed \n",
      "Research International 2015, 1–9 (2015).\n",
      "\n",
      "Result 4:\n",
      "ods are best for large and diverse datasets, LLMs can provide\n",
      "comparable results when analyzing small and homogeneous\n",
      "datasets [25].\n",
      "With this introduction, we propose a multilayered network\n",
      "medicine approach, accelerated by ChatGPT few-shot prompt\n",
      "engineering, that streamlines data processing and network\n",
      "layer construction without compromising the accuracy of\n",
      "results.\n",
      "II. DATA\n",
      "This research used three different type of resources:\n",
      "1.\n",
      "Clinical Trials as a Source of Real-World Evidence:\n",
      "We searched for drug combinations using the query ”drug\n",
      "combination” and acquired approximately 2,450 trials\n",
      "that mentioned treatments involving more than one drug.\n",
      "The trials were extracted in JSON format, with each trial\n",
      "stored in an independent file.\n",
      "2.\n",
      "KEGG Breast Cancer Signaling Pathways as Potential\n",
      "Targets: The signaling pathways serve two purposes:\n",
      "first, providing a list of relevant biological targets; second,\n",
      "identifying drugs that may bind with proteins in the same\n",
      "\n",
      "Result 5:\n",
      "embeddings. A scoring layer then computes the similarity between every label and every entity pair. Diagram\n",
      "adapted from Zaratiana et al. (2023).\n",
      "factorization raises issues around reliability. For\n",
      "information extraction tasks in particular, assigning\n",
      "relationships between all entities found in a text is\n",
      "a requirement that cannot be achieved reliably with\n",
      "LLMs without auxiliary models (Li et al., 2024b)\n",
      "and/or at-runtime data augmentation mechanisms\n",
      "(Jiang et al., 2024; Ma et al., 2023b).\n",
      "In contrast to the disadvantages above, LLMs\n",
      "excel at unconstrained classification and labeling\n",
      "tasks, and can be effectively utilized to produce\n",
      "large scale datasets for training downstream mod-\n",
      "els with task-specific inductive biases. Synthetic\n",
      "dataset generation using general-purpose LLMs is\n",
      "a critical component of the recent success of zero-\n",
      "shot NLP models (Zaratiana et al., 2023; Bogdanov\n",
      "et al., 2024). This paper includes a protocol for gen-\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform a query to retrieve the most relevant chunks\n",
    "query_text = \"Using LLMs for drug discovery\"\n",
    "\n",
    "# Perform the search\n",
    "results = qdrant.similarity_search(query_text, k=5)  # Retrieve top 5 results\n",
    "\n",
    "# Display results\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"Result {i}:\\n{result.page_content}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Only run when initializing an empty vector store collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all files in the S3 bucket using pagination\n",
    "# paginator = s3_client.get_paginator('list_objects_v2')\n",
    "# pages = paginator.paginate(Bucket=bucket_name)\n",
    "\n",
    "# Process each file in the S3 bucket\n",
    "for page in pages:\n",
    "    for obj in page.get('Contents', []):\n",
    "        file_key = obj['Key']\n",
    "        \n",
    "        # Skip non-PDF files\n",
    "        if not file_key.lower().endswith('.pdf'):\n",
    "            print(f\"Skipping non-PDF file: {file_key}\")\n",
    "            continue\n",
    "\n",
    "        # Download the file from S3 to a temporary location\n",
    "        temp_file_path = f\"/tmp/{os.path.basename(file_key)}\"\n",
    "        try:\n",
    "            s3_client.download_file(bucket_name, file_key, temp_file_path)\n",
    "            print(f\"Downloaded {file_key} to {temp_file_path}.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to download {file_key}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Load the PDF document\n",
    "        try:\n",
    "            loader = PyMuPDFLoader(temp_file_path)\n",
    "            docs = loader.load()\n",
    "            print(f\"Loaded {file_key} with {len(docs)} pages.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load {file_key}: {e}\")\n",
    "            os.remove(temp_file_path)\n",
    "            continue\n",
    "\n",
    "        # Split the document into chunks\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=CHUNK_SIZE,\n",
    "            chunk_overlap=CHUNK_OVERLAP\n",
    "        )\n",
    "        splits = text_splitter.split_documents(docs)\n",
    "        print(f\"Split {file_key} into {len(splits)} chunks.\")\n",
    "\n",
    "        # Store the embeddings in Qdrant\n",
    "        try:\n",
    "            qdrant = QdrantVectorStore.from_documents(\n",
    "                documents=splits,\n",
    "                embedding=embedding_model,\n",
    "                collection_name=COLLECTION_NAME,\n",
    "                url=QDRANT_API_URL,\n",
    "                api_key=QDRANT_API_KEY,\n",
    "                prefer_grpc=True\n",
    "            )\n",
    "            print(f\"Successfully stored embeddings for {file_key} in Qdrant.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to store embeddings for {file_key}: {e}\")\n",
    "\n",
    "        # Clean up the temporary file\n",
    "        os.remove(temp_file_path)\n",
    "        print(f\"Deleted temporary file {temp_file_path}.\")\n",
    "\n",
    "print(\"All files have been processed and stored in Qdrant.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
