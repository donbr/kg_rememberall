{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMJYRc1uY-se"
      },
      "source": [
        "# Breaking Graph Visualization of Arxiv Papers into components\n",
        "\n",
        "1. Generating the NetworkX Graph\n",
        "1. Visualizing Using PyVis\n",
        "1. Visualizing Using Bokeh\n",
        "1. Exporting as GraphML\n",
        "1. Exporting as JSON"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6RiF2YDfa28X",
        "outputId": "d6950169-104d-42aa-c644-51c4f9750ac6"
      },
      "outputs": [],
      "source": [
        "# install python libraries\n",
        "\n",
        "%pip install -q networkx pyvis bokeh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-RQ-_zRZRQl"
      },
      "source": [
        "## Generating the NetworkX Graph\n",
        "\n",
        "This function creates and returns a NetworkX graph with nodes, edges, and attributes.[link text](https://)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_W54yLEtZO-6"
      },
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "\n",
        "def create_networkx_graph():\n",
        "    # Create a directed graph\n",
        "    G = nx.DiGraph()\n",
        "\n",
        "    # Define color schemes for nodes\n",
        "    COLOR_SCHEME = {\n",
        "        \"section\": \"lightblue\",  # Blue for sections\n",
        "        \"subsection\": \"lightgreen\",  # Green for subsections\n",
        "    }\n",
        "\n",
        "    # Add nodes for the main sections with summaries as properties\n",
        "    sections = [\n",
        "        (\"Abstract\", \"Overview of LightRAG's novel approach for retrieval-augmented generation using graph structures.\"),\n",
        "        (\"1 Introduction\", \"Motivation and importance of enhancing retrieval-augmented generation with graph-based indexing.\"),\n",
        "        (\"2 Retrieval-Augmented Generation\", \"Discussion of the RAG framework, including retrieval and generation components.\"),\n",
        "        (\"3 The LightRAG Architecture\", \"Details of LightRAG's architecture, including graph-based text indexing and dual-level retrieval.\"),\n",
        "        (\"4 Evaluation\", \"Empirical evaluation of LightRAG using benchmark datasets and comparison with baseline methods.\"),\n",
        "        (\"5 Related Work\", \"Discussion of related work in retrieval-augmented generation and large language models for graphs.\"),\n",
        "        (\"6 Conclusion\", \"Summary of findings and LightRAG's contributions.\"),\n",
        "        (\"References\", \"List of cited works and related research.\"),\n",
        "        (\"Appendix\", \"Supplementary material, including extended results and technical details.\")\n",
        "    ]\n",
        "\n",
        "    for section, summary in sections:\n",
        "        G.add_node(section, type=\"section\", summary=summary, color=COLOR_SCHEME[\"section\"])\n",
        "\n",
        "    # Add edges to represent the flow of the document\n",
        "    for i in range(len(sections) - 1):\n",
        "        G.add_edge(sections[i][0], sections[i + 1][0], transition_summary=f\"Transition from {sections[i][0]} to {sections[i + 1][0]}.\")\n",
        "\n",
        "    # Add subsections with summaries as properties\n",
        "    subsections = {\n",
        "        \"2 Retrieval-Augmented Generation\": [\n",
        "            (\"Comprehensive Information Retrieval\", \"The indexing function must extract global information for effective query answering.\"),\n",
        "            (\"Efficient and Low-Cost Retrieval\", \"The indexed data structure must enable rapid and cost-efficient retrieval.\"),\n",
        "            (\"Fast Adaptation to Data Changes\", \"The system must quickly adapt to new information from the external knowledge base.\")\n",
        "        ],\n",
        "        \"3 The LightRAG Architecture\": [\n",
        "            (\"Graph-based Text Indexing\", \"Details the process of extracting entities and relationships using LLMs and constructing a knowledge graph.\"),\n",
        "            (\"Dual-level Retrieval Paradigm\", \"Explains the low-level and high-level retrieval strategies for specific and abstract queries.\"),\n",
        "            (\"Retrieval-Augmented Answer Generation\", \"Describes how retrieved information is used by the LLM to generate contextually relevant answers.\"),\n",
        "            (\"Complexity Analysis\", \"Analyzes the computational complexity of LightRAG's indexing and retrieval processes.\")\n",
        "        ],\n",
        "        \"4 Evaluation\": [\n",
        "            (\"Experimental Settings\", \"Describes the datasets, question generation, baselines, and evaluation metrics used in the experiments.\"),\n",
        "            (\"Comparison of LightRAG with Existing RAG Methods\", \"Presents the results of LightRAG compared to baseline methods across various datasets.\"),\n",
        "            (\"Ablation Studies\", \"Examines the impact of dual-level retrieval and graph-based indexing on LightRAG's performance.\"),\n",
        "            (\"Case Study\", \"Provides specific examples comparing LightRAG with baseline methods.\"),\n",
        "            (\"Model Cost and Adaptability Analysis\", \"Analyzes the cost and adaptability of LightRAG in dynamic environments.\")\n",
        "        ],\n",
        "        \"5 Related Work\": [\n",
        "            (\"Retrieval-Augmented Generation with LLMs\", \"Reviews existing RAG approaches and their limitations.\"),\n",
        "            (\"Large Language Model for Graphs\", \"Explores the integration of LLMs with graph-structured data.\")\n",
        "        ],\n",
        "        \"Appendix\": [\n",
        "            (\"Experimental Data Details\", \"Provides statistical information about the datasets used in the experiments.\"),\n",
        "            (\"Case Example of Retrieval-Augmented Generation in LightRAG\", \"Illustrates the retrieve-and-generate process with an example query.\"),\n",
        "            (\"Overview of the Prompts Used in LightRAG\", \"Details the prompts used for graph generation, query generation, keyword extraction, and RAG evaluation.\"),\n",
        "            (\"Case Study: Comparison Between LightRAG and the Baseline NaiveRAG\", \"Presents a case study comparing LightRAG with NaiveRAG.\")\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    for section, subs in subsections.items():\n",
        "        for sub, sub_summary in subs:\n",
        "            G.add_node(sub, type=\"subsection\", summary=sub_summary, color=COLOR_SCHEME[\"subsection\"])\n",
        "            G.add_edge(section, sub, transition_summary=f\"Transition from {section} to {sub}.\")\n",
        "\n",
        "    # Add edges between subsections where applicable\n",
        "    G.add_edge(\"2 Retrieval-Augmented Generation\", \"3 The LightRAG Architecture\", transition_summary=\"Transition from RAG framework to LightRAG architecture.\")\n",
        "    G.add_edge(\"3 The LightRAG Architecture\", \"4 Evaluation\", transition_summary=\"Transition from LightRAG architecture to evaluation.\")\n",
        "    G.add_edge(\"4 Evaluation\", \"5 Related Work\", transition_summary=\"Transition from evaluation to related work.\")\n",
        "    G.add_edge(\"5 Related Work\", \"6 Conclusion\", transition_summary=\"Transition from related work to conclusion.\")\n",
        "    G.add_edge(\"6 Conclusion\", \"References\", transition_summary=\"Transition from conclusion to references.\")\n",
        "    G.add_edge(\"References\", \"Appendix\", transition_summary=\"Transition from references to appendix.\")\n",
        "\n",
        "    return G"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RveM3_ShZi5S"
      },
      "source": [
        "## Visualizing Using PyVis\n",
        "\n",
        "This function visualizes the graph using the pyvis library.\n",
        "\n",
        "- certain features only work with the \"save to file\" version below.  (text hints, etc.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilnLVBmIVlD4"
      },
      "source": [
        "### display in notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "id": "nTtDTCzqV8W3",
        "outputId": "1461887b-49ce-486f-f0ec-2b8cbd65441f"
      },
      "outputs": [],
      "source": [
        "from pyvis.network import Network\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "# Create the graph\n",
        "G = create_networkx_graph()\n",
        "\n",
        "# Convert NetworkX graph to PyVis\n",
        "net = Network(notebook=True, height=\"600px\", width=\"100%\", cdn_resources='in_line')  # Use 'in_line' for Colab\n",
        "net.from_nx(G)\n",
        "\n",
        "# Customize node and edge appearance\n",
        "for node in net.nodes:\n",
        "    node[\"color\"] = G.nodes[node[\"id\"]][\"color\"]  # Set node color\n",
        "    node[\"size\"] = 20  # Set node size\n",
        "\n",
        "# Generate the HTML file\n",
        "net.save_graph(\"graph.html\")\n",
        "\n",
        "# Read the HTML file and display it in the notebook\n",
        "with open(\"graph.html\", \"r\", encoding=\"utf-8\") as f:\n",
        "    html_content = f.read()\n",
        "\n",
        "display(HTML(html_content))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHw5JuYGV9UE"
      },
      "source": [
        "### save to file\n",
        "\n",
        "- text hints work consistently when downloaded to desktop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n_aSCvXUlbtm"
      },
      "outputs": [],
      "source": [
        "from pyvis.network import Network\n",
        "\n",
        "def visualize_with_pyvis(G):\n",
        "    # Create a PyVis network\n",
        "    net = Network(notebook=True, directed=True, height=\"750px\", width=\"100%\", cdn_resources='in_line')\n",
        "\n",
        "    # Add nodes and edges from the NetworkX graph\n",
        "    for node in G.nodes:\n",
        "        net.add_node(node, label=node, color=G.nodes[node][\"color\"], title=G.nodes[node][\"summary\"])\n",
        "\n",
        "    for edge in G.edges:\n",
        "        net.add_edge(edge[0], edge[1], title=G.edges[edge][\"transition_summary\"])\n",
        "\n",
        "    # Customize the visualization\n",
        "    net.toggle_physics(True)  # Enable physics for better layout\n",
        "\n",
        "    # Enable navigation buttons by default\n",
        "    net.set_options(\"\"\"\n",
        "    {\n",
        "      \"interaction\": {\n",
        "        \"navigationButtons\": true\n",
        "      }\n",
        "    }\n",
        "    \"\"\")\n",
        "\n",
        "    # Show the graph\n",
        "    net.show(\"document_structure_pyvis.html\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcSqRrOAZxFG"
      },
      "source": [
        "## Visualizing Using Bokeh\n",
        "\n",
        "This function visualizes the graph using the bokeh library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cGtGj2otlhv_"
      },
      "outputs": [],
      "source": [
        "from bokeh.io import show, output_file\n",
        "from bokeh.plotting import figure\n",
        "from bokeh.models import GraphRenderer, StaticLayoutProvider, Circle, ColumnDataSource, MultiLine, HoverTool, LabelSet\n",
        "\n",
        "def visualize_with_bokeh(G):\n",
        "    # Create a Bokeh plot\n",
        "    plot = figure(\n",
        "        title=\"Document Structure\",\n",
        "        x_range=(-1.5, 1.5),\n",
        "        y_range=(-1.5, 1.5),\n",
        "        tools=\"pan,wheel_zoom,box_zoom,reset,hover\",\n",
        "        toolbar_location=\"below\",\n",
        "    )\n",
        "\n",
        "    # Convert NetworkX graph to Bokeh GraphRenderer\n",
        "    graph = GraphRenderer()\n",
        "\n",
        "    # Add nodes and edges to the Bokeh graph\n",
        "    graph.node_renderer.data_source.data = {\n",
        "        \"index\": list(G.nodes),\n",
        "        \"type\": [G.nodes[node][\"type\"] for node in G.nodes],\n",
        "        \"summary\": [G.nodes[node][\"summary\"] for node in G.nodes],\n",
        "        \"color\": [G.nodes[node][\"color\"] for node in G.nodes],\n",
        "        \"x\": [0] * len(G.nodes),  # Placeholder for x-coordinates\n",
        "        \"y\": [0] * len(G.nodes),  # Placeholder for y-coordinates\n",
        "    }\n",
        "\n",
        "    graph.edge_renderer.data_source.data = {\n",
        "        \"start\": [edge[0] for edge in G.edges],\n",
        "        \"end\": [edge[1] for edge in G.edges],\n",
        "        \"transition_summary\": [G.edges[edge][\"transition_summary\"] for edge in G.edges],\n",
        "    }\n",
        "\n",
        "    # Use a spring layout to position nodes\n",
        "    pos = nx.spring_layout(G, seed=42)\n",
        "    graph_layout = {node: (pos[node][0], pos[node][1]) for node in G.nodes}\n",
        "    graph.layout_provider = StaticLayoutProvider(graph_layout=graph_layout)\n",
        "\n",
        "    # Update node positions in the data source\n",
        "    graph.node_renderer.data_source.data[\"x\"] = [pos[node][0] for node in G.nodes]\n",
        "    graph.node_renderer.data_source.data[\"y\"] = [pos[node][1] for node in G.nodes]\n",
        "\n",
        "    # Style nodes\n",
        "    graph.node_renderer.glyph = Circle(radius=0.1, fill_color=\"color\", line_color=\"black\")\n",
        "\n",
        "    # Style edges\n",
        "    graph.edge_renderer.glyph = MultiLine(line_color=\"#CCCCCC\", line_alpha=0.8, line_width=2)\n",
        "\n",
        "    # Add hover tooltips\n",
        "    hover = HoverTool(\n",
        "        tooltips=[\n",
        "            (\"Node\", \"@index\"),\n",
        "            (\"Type\", \"@type\"),\n",
        "            (\"Summary\", \"@summary\"),\n",
        "        ],\n",
        "        line_policy=\"interp\",\n",
        "    )\n",
        "    plot.add_tools(hover)\n",
        "\n",
        "    # Add node labels\n",
        "    labels = LabelSet(\n",
        "        x=\"x\",\n",
        "        y=\"y\",\n",
        "        text=\"index\",\n",
        "        source=graph.node_renderer.data_source,\n",
        "        text_font_size=\"10pt\",\n",
        "        text_color=\"black\",\n",
        "        x_offset=5,\n",
        "        y_offset=5,\n",
        "    )\n",
        "    plot.add_layout(labels)\n",
        "\n",
        "    # Add the graph to the plot\n",
        "    plot.renderers.append(graph)\n",
        "\n",
        "    # Output to HTML file\n",
        "    output_file(\"document_structure_bokeh.html\")\n",
        "\n",
        "    # Show the plot\n",
        "    show(plot)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwejt45_Z4YE"
      },
      "source": [
        "## Exporting as GraphML\n",
        "This function exports the graph as a GraphML file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ILv40OYVZ7zV"
      },
      "outputs": [],
      "source": [
        "def export_as_graphml(G, filename=\"document_structure.graphml\"):\n",
        "    nx.write_graphml(G, filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evD4WloVZ_SS"
      },
      "source": [
        "## Exporting as JSON\n",
        "\n",
        "This function exports the graph as a JSON file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WrlMKqdsaDeo"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def export_as_json(G, filename=\"document_structure.json\"):\n",
        "    # Convert the graph to a dictionary\n",
        "    graph_data = nx.node_link_data(G, edges=\"edges\")\n",
        "\n",
        "    # Write to JSON file\n",
        "    with open(filename, \"w\") as f:\n",
        "        json.dump(graph_data, f, indent=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEyjHqL_aKPS"
      },
      "source": [
        "## Putting It All Together\n",
        "\n",
        "You can now call these functions in sequence to generate, visualize, and export the graph:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhJF2yo5aNY_",
        "outputId": "d49bbc51-dfb1-4980-9b73-1fc394eaf3d5"
      },
      "outputs": [],
      "source": [
        "# Step 1: Create the NetworkX graph\n",
        "G = create_networkx_graph()\n",
        "\n",
        "# Step 2: Visualize with PyVis\n",
        "visualize_with_pyvis(G)\n",
        "\n",
        "# Step 3: Visualize with Bokeh\n",
        "visualize_with_bokeh(G)\n",
        "\n",
        "# Step 4: Export as GraphML\n",
        "export_as_graphml(G)\n",
        "\n",
        "# Step 5: Export as JSON\n",
        "export_as_json(G)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4AUE1AwZSJQ"
      },
      "source": [
        "## Experiment with additional detail"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GRfWAyrq18xu"
      },
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "\n",
        "def create_networkx_graph():\n",
        "    # Create a directed graph\n",
        "    G = nx.DiGraph()\n",
        "\n",
        "    # Define color schemes for nodes\n",
        "    COLOR_SCHEME = {\n",
        "        \"section\": \"lightblue\",  # Blue for sections\n",
        "        \"subsection\": \"lightgreen\",  # Green for subsections\n",
        "    }\n",
        "\n",
        "    # Add nodes for the main sections with summaries as properties\n",
        "    sections = [\n",
        "        (\"Abstract\", {\n",
        "            \"summary\": \"Overview of LightRAG's novel approach for retrieval-augmented generation using graph structures.\",\n",
        "            \"type\": \"section\",\n",
        "            \"color\": COLOR_SCHEME[\"section\"]\n",
        "        }),\n",
        "        (\"1 Introduction\", {\n",
        "            \"summary\": \"Motivation and importance of enhancing retrieval-augmented generation with graph-based indexing.\",\n",
        "            \"type\": \"section\",\n",
        "            \"color\": COLOR_SCHEME[\"section\"]\n",
        "        }),\n",
        "        (\"2 Retrieval-Augmented Generation\", {\n",
        "            \"summary\": \"Discussion of the RAG framework, including retrieval and generation components.\",\n",
        "            \"type\": \"section\",\n",
        "            \"color\": COLOR_SCHEME[\"section\"]\n",
        "        }),\n",
        "        (\"3 The LightRAG Architecture\", {\n",
        "            \"summary\": \"Details of LightRAG's architecture, including graph-based text indexing and dual-level retrieval.\",\n",
        "            \"type\": \"section\",\n",
        "            \"color\": COLOR_SCHEME[\"section\"]\n",
        "        }),\n",
        "        (\"4 Evaluation\", {\n",
        "            \"summary\": \"Empirical evaluation of LightRAG using benchmark datasets and comparison with baseline methods.\",\n",
        "            \"type\": \"section\",\n",
        "            \"color\": COLOR_SCHEME[\"section\"]\n",
        "        }),\n",
        "        (\"5 Related Work\", {\n",
        "            \"summary\": \"Discussion of related work in retrieval-augmented generation and large language models for graphs.\",\n",
        "            \"type\": \"section\",\n",
        "            \"color\": COLOR_SCHEME[\"section\"]\n",
        "        }),\n",
        "        (\"6 Conclusion\", {\n",
        "            \"summary\": \"Summary of findings and LightRAG's contributions.\",\n",
        "            \"type\": \"section\",\n",
        "            \"color\": COLOR_SCHEME[\"section\"]\n",
        "        }),\n",
        "        (\"References\", {\n",
        "            \"summary\": \"List of cited works and related research.\",\n",
        "            \"type\": \"section\",\n",
        "            \"color\": COLOR_SCHEME[\"section\"]\n",
        "        }),\n",
        "        (\"Appendix\", {\n",
        "            \"summary\": \"Supplementary material, including extended results and technical details.\",\n",
        "            \"type\": \"section\",\n",
        "            \"color\": COLOR_SCHEME[\"section\"]\n",
        "        })\n",
        "    ]\n",
        "\n",
        "    for section, properties in sections:\n",
        "        G.add_node(section, **properties)\n",
        "\n",
        "    # Add edges to represent the flow of the document\n",
        "    for i in range(len(sections) - 1):\n",
        "        G.add_edge(sections[i][0], sections[i + 1][0], transition_summary=f\"Transition from {sections[i][0]} to {sections[i + 1][0]}.\")\n",
        "\n",
        "    # Add subsections with summaries as properties\n",
        "    subsections = {\n",
        "        \"2 Retrieval-Augmented Generation\": [\n",
        "            (\"Comprehensive Information Retrieval\", {\n",
        "                \"summary\": \"The indexing function must extract global information for effective query answering.\",\n",
        "                \"type\": \"subsection\",\n",
        "                \"color\": COLOR_SCHEME[\"subsection\"]\n",
        "            }),\n",
        "            (\"Efficient and Low-Cost Retrieval\", {\n",
        "                \"summary\": \"The indexed data structure must enable rapid and cost-efficient retrieval.\",\n",
        "                \"type\": \"subsection\",\n",
        "                \"color\": COLOR_SCHEME[\"subsection\"]\n",
        "            }),\n",
        "            (\"Fast Adaptation to Data Changes\", {\n",
        "                \"summary\": \"The system must quickly adapt to new information from the external knowledge base.\",\n",
        "                \"type\": \"subsection\",\n",
        "                \"color\": COLOR_SCHEME[\"subsection\"]\n",
        "            })\n",
        "        ],\n",
        "        \"3 The LightRAG Architecture\": [\n",
        "            (\"Graph-based Text Indexing\", {\n",
        "                \"summary\": \"Details the process of extracting entities and relationships using LLMs and constructing a knowledge graph.\",\n",
        "                \"type\": \"subsection\",\n",
        "                \"color\": COLOR_SCHEME[\"subsection\"]\n",
        "            }),\n",
        "            (\"Dual-level Retrieval Paradigm\", {\n",
        "                \"summary\": \"Explains the low-level and high-level retrieval strategies for specific and abstract queries.\",\n",
        "                \"type\": \"subsection\",\n",
        "                \"color\": COLOR_SCHEME[\"subsection\"]\n",
        "            }),\n",
        "            (\"Retrieval-Augmented Answer Generation\", {\n",
        "                \"summary\": \"Describes how retrieved information is used by the LLM to generate contextually relevant answers.\",\n",
        "                \"type\": \"subsection\",\n",
        "                \"color\": COLOR_SCHEME[\"subsection\"]\n",
        "            }),\n",
        "            (\"Complexity Analysis\", {\n",
        "                \"summary\": \"Analyzes the computational complexity of LightRAG's indexing and retrieval processes.\",\n",
        "                \"type\": \"subsection\",\n",
        "                \"color\": COLOR_SCHEME[\"subsection\"]\n",
        "            })\n",
        "        ],\n",
        "        \"4 Evaluation\": [\n",
        "            (\"Experimental Settings\", {\n",
        "                \"summary\": \"Describes the datasets, question generation, baselines, and evaluation metrics used in the experiments.\",\n",
        "                \"type\": \"subsection\",\n",
        "                \"color\": COLOR_SCHEME[\"subsection\"]\n",
        "            }),\n",
        "            (\"Comparison of LightRAG with Existing RAG Methods\", {\n",
        "                \"summary\": \"Presents the results of LightRAG compared to baseline methods across various datasets.\",\n",
        "                \"type\": \"subsection\",\n",
        "                \"color\": COLOR_SCHEME[\"subsection\"]\n",
        "            }),\n",
        "            (\"Ablation Studies\", {\n",
        "                \"summary\": \"Examines the impact of dual-level retrieval and graph-based indexing on LightRAG's performance.\",\n",
        "                \"type\": \"subsection\",\n",
        "                \"color\": COLOR_SCHEME[\"subsection\"]\n",
        "            }),\n",
        "            (\"Case Study\", {\n",
        "                \"summary\": \"Provides specific examples comparing LightRAG with baseline methods.\",\n",
        "                \"type\": \"subsection\",\n",
        "                \"color\": COLOR_SCHEME[\"subsection\"]\n",
        "            }),\n",
        "            (\"Model Cost and Adaptability Analysis\", {\n",
        "                \"summary\": \"Analyzes the cost and adaptability of LightRAG in dynamic environments.\",\n",
        "                \"type\": \"subsection\",\n",
        "                \"color\": COLOR_SCHEME[\"subsection\"]\n",
        "            })\n",
        "        ],\n",
        "        \"5 Related Work\": [\n",
        "            (\"Retrieval-Augmented Generation with LLMs\", {\n",
        "                \"summary\": \"Reviews existing RAG approaches and their limitations.\",\n",
        "                \"type\": \"subsection\",\n",
        "                \"color\": COLOR_SCHEME[\"subsection\"]\n",
        "            }),\n",
        "            (\"Large Language Model for Graphs\", {\n",
        "                \"summary\": \"Explores the integration of LLMs with graph-structured data.\",\n",
        "                \"type\": \"subsection\",\n",
        "                \"color\": COLOR_SCHEME[\"subsection\"]\n",
        "            })\n",
        "        ],\n",
        "        \"Appendix\": [\n",
        "            (\"Experimental Data Details\", {\n",
        "                \"summary\": \"Provides statistical information about the datasets used in the experiments.\",\n",
        "                \"type\": \"subsection\",\n",
        "                \"color\": COLOR_SCHEME[\"subsection\"]\n",
        "            }),\n",
        "            (\"Case Example of Retrieval-Augmented Generation in LightRAG\", {\n",
        "                \"summary\": \"Illustrates the retrieve-and-generate process with an example query.\",\n",
        "                \"type\": \"subsection\",\n",
        "                \"color\": COLOR_SCHEME[\"subsection\"]\n",
        "            }),\n",
        "            (\"Overview of the Prompts Used in LightRAG\", {\n",
        "                \"summary\": \"Details the prompts used for graph generation, query generation, keyword extraction, and RAG evaluation.\",\n",
        "                \"type\": \"subsection\",\n",
        "                \"color\": COLOR_SCHEME[\"subsection\"]\n",
        "            }),\n",
        "            (\"Case Study: Comparison Between LightRAG and the Baseline NaiveRAG\", {\n",
        "                \"summary\": \"Presents a case study comparing LightRAG with NaiveRAG.\",\n",
        "                \"type\": \"subsection\",\n",
        "                \"color\": COLOR_SCHEME[\"subsection\"]\n",
        "            })\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    for section, subs in subsections.items():\n",
        "        for sub, properties in subs:\n",
        "            G.add_node(sub, **properties)\n",
        "            G.add_edge(section, sub, transition_summary=f\"Transition from {section} to {sub}.\")\n",
        "\n",
        "    # Add edges between subsections where applicable\n",
        "    G.add_edge(\"2 Retrieval-Augmented Generation\", \"3 The LightRAG Architecture\", transition_summary=\"Transition from RAG framework to LightRAG architecture.\")\n",
        "    G.add_edge(\"3 The LightRAG Architecture\", \"4 Evaluation\", transition_summary=\"Transition from LightRAG architecture to evaluation.\")\n",
        "    G.add_edge(\"4 Evaluation\", \"5 Related Work\", transition_summary=\"Transition from evaluation to related work.\")\n",
        "    G.add_edge(\"5 Related Work\", \"6 Conclusion\", transition_summary=\"Transition from related work to conclusion.\")\n",
        "    G.add_edge(\"6 Conclusion\", \"References\", transition_summary=\"Transition from conclusion to references.\")\n",
        "    G.add_edge(\"References\", \"Appendix\", transition_summary=\"Transition from references to appendix.\")\n",
        "\n",
        "    return G"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "id": "TWV0xICHagve",
        "outputId": "7c2f9d48-47c7-46c3-b52e-bc60c7171f77"
      },
      "outputs": [],
      "source": [
        "from pyvis.network import Network\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "# Create the graph\n",
        "G = create_networkx_graph()\n",
        "\n",
        "# Convert NetworkX graph to PyVis\n",
        "net = Network(notebook=True, height=\"600px\", width=\"100%\", cdn_resources='in_line')  # Use 'in_line' for Colab\n",
        "net.from_nx(G)\n",
        "\n",
        "# Customize node appearance and tooltips\n",
        "for node in net.nodes:\n",
        "    node_id = node[\"id\"]\n",
        "    node[\"color\"] = G.nodes[node_id][\"color\"]  # Set node color\n",
        "    node[\"size\"] = 20  # Set node size\n",
        "\n",
        "    # Create a Markdown-like tooltip using HTML\n",
        "    node[\"title\"] = f\"\"\"\n",
        "    Entity: {node_id}\n",
        "    Type: {G.nodes[node_id]['type']}\n",
        "    Summary: {G.nodes[node_id]['summary']}\n",
        "    \"\"\"\n",
        "\n",
        "# Customize edge appearance and tooltips\n",
        "for edge in net.edges:\n",
        "    source, target = edge[\"from\"], edge[\"to\"]\n",
        "    edge[\"color\"] = \"gray\"  # Set edge color\n",
        "    edge[\"width\"] = 2  # Set edge width\n",
        "\n",
        "    # Create a Markdown-like tooltip using HTML\n",
        "    edge[\"title\"] = f\"\"\"\n",
        "    Transition: {source} → {target}\n",
        "    Summary: {G.edges[source, target]['transition_summary']}\n",
        "    \"\"\"\n",
        "\n",
        "# Enable physics for interactive visualization\n",
        "net.toggle_physics(True)\n",
        "\n",
        "# Generate the HTML file\n",
        "net.save_graph(\"graph.html\")\n",
        "\n",
        "# Read the HTML file and display it in the notebook\n",
        "with open(\"graph.html\", \"r\", encoding=\"utf-8\") as f:\n",
        "    html_content = f.read()\n",
        "\n",
        "# Display the HTML content in the notebook\n",
        "display(HTML(html_content))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X0ZzAlTycEi4"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
